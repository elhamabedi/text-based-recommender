{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecd3889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading restaurant reviews...\n",
      "Total restaurant reviews: 4724471\n",
      "Filtering users and restaurants...\n",
      "Users with at least 100 restaurant reviews: 2121\n",
      "Restaurants with at least 1000 reviewing users: 296\n",
      "Filtered reviews: 23924\n",
      "\n",
      "Number of Records:\n",
      "Users\tRestaurants\tReviews\n",
      "2121\t296\t23924\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_restaurant_data(business_path, review_path):\n",
    "    # Identify restaurant business IDs\n",
    "    restaurants = set()\n",
    "    with open(business_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            business = json.loads(line)\n",
    "            if business.get('categories') and 'Restaurants' in str(business['categories']):\n",
    "                restaurants.add(business['business_id'])\n",
    "\n",
    "    # Collect reviews only for restaurants\n",
    "    review_list = []\n",
    "    with open(review_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            review = json.loads(line)\n",
    "            if review['business_id'] in restaurants:\n",
    "                review_list.append({\n",
    "                    'user_id': review['user_id'],\n",
    "                    'business_id': review['business_id'],\n",
    "                    'text': review['text'],\n",
    "                    'stars': review['stars'],\n",
    "                    'date': review['date']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(review_list)\n",
    "\n",
    "def filter_entities(dataframe):\n",
    "    # Remove duplicate reviews (one review per user per restaurant)\n",
    "    unique_data = dataframe[['user_id', 'business_id']].drop_duplicates()\n",
    "\n",
    "    # Count the number of restaurants reviewed by each user\n",
    "    user_reviews = defaultdict(int)\n",
    "    for user in unique_data['user_id']:\n",
    "        user_reviews[user] += 1\n",
    "    selected_users = {user for user, count in user_reviews.items() if count >= 100}\n",
    "\n",
    "    # Count the number of users reviewing each restaurant\n",
    "    restaurant_reviews = defaultdict(int)\n",
    "    for rest in unique_data['business_id']:\n",
    "        restaurant_reviews[rest] += 1\n",
    "    selected_restaurants = {rest for rest, count in restaurant_reviews.items() if count >= 1000}\n",
    "\n",
    "    # Filter reviews for valid users and restaurants\n",
    "    filtered_data = dataframe[\n",
    "        dataframe['user_id'].isin(selected_users) & \n",
    "        dataframe['business_id'].isin(selected_restaurants)\n",
    "    ].copy()\n",
    "\n",
    "    # Create DataFrames for users and restaurants\n",
    "    users_df = pd.DataFrame({'user_id': list(selected_users)})\n",
    "    restaurants_df = pd.DataFrame({'business_id': list(selected_restaurants)})\n",
    "\n",
    "    return users_df, restaurants_df, filtered_data\n",
    "\n",
    "def process_yelp_data(users_path, businesses_path, reviews_path, output_dir='dataset'):\n",
    "    print(\"Loading restaurant reviews...\")\n",
    "    reviews_df = load_restaurant_data(businesses_path, reviews_path)\n",
    "    print(f\"Total restaurant reviews: {len(reviews_df)}\")\n",
    "\n",
    "    print(\"Filtering users and restaurants...\")\n",
    "    users, restaurants, filtered_reviews = filter_entities(reviews_df)\n",
    "    print(f\"Users with at least 100 restaurant reviews: {len(users)}\")\n",
    "    print(f\"Restaurants with at least 1000 reviewing users: {len(restaurants)}\")\n",
    "    print(f\"Filtered reviews: {len(filtered_reviews)}\")\n",
    "\n",
    "    # Display report table\n",
    "    print(\"\\nNumber of Records:\")\n",
    "    print(\"Users\\tRestaurants\\tReviews\")\n",
    "    print(f\"{len(users)}\\t{len(restaurants)}\\t{len(filtered_reviews)}\")\n",
    "\n",
    "    # Save to CSV files in the dataset directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    users.to_csv(os.path.join(output_dir, 'filtered_users.csv'), index=False)\n",
    "    restaurants.to_csv(os.path.join(output_dir, 'filtered_restaurants.csv'), index=False)\n",
    "    filtered_reviews.to_csv(os.path.join(output_dir, 'filtered_reviews.csv'), index=False)\n",
    "\n",
    "    return users, restaurants, filtered_reviews\n",
    "\n",
    "# File paths\n",
    "data_dir = 'dataset'\n",
    "users_path = os.path.join(data_dir, 'yelp_academic_dataset_user.json')\n",
    "businesses_path = os.path.join(data_dir, 'yelp_academic_dataset_business.json')\n",
    "reviews_path = os.path.join(data_dir, 'yelp_academic_dataset_review.json')\n",
    "\n",
    "# Execute processing\n",
    "try:\n",
    "    users_df, restaurants_df, reviews_df = process_yelp_data(users_path, businesses_path, reviews_path)\n",
    "except FileNotFoundError as err:\n",
    "    print(f\"File error: {err}.Please ensure the JSON files exist in the specified paths.\")\n",
    "except Exception as err:\n",
    "    print(f\"unexpected error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d512a8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1.2: Holdout Split\n",
      "Training set size: 20000\n",
      "Validation set size: 1962\n",
      "Test set size: 1962\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "\n",
    "print(\"Section 1.2: Holdout Split\")\n",
    "\n",
    "# Load the filtered reviews dataset\n",
    "try:\n",
    "    filtered_reviews = pd.read_csv(\"dataset/filtered_reviews.csv\")\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(\"Error: filtered_reviews.csv not found. Please run Section 1.1 first.\")\n",
    "\n",
    "# Ensure 'date' column is in datetime format\n",
    "if 'date' in filtered_reviews.columns:\n",
    "    filtered_reviews['date'] = pd.to_datetime(filtered_reviews['date'])\n",
    "else:\n",
    "    raise SystemExit(\"Error: 'date' column not found in filtered_reviews.csv\")\n",
    "\n",
    "# Sort by date (oldest first)\n",
    "filtered_reviews = filtered_reviews.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "filtered_reviews['text'] = filtered_reviews['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data\n",
    "train_size = 20000\n",
    "train = filtered_reviews.iloc[:train_size]\n",
    "remaining = filtered_reviews.iloc[train_size:]\n",
    "\n",
    "# Split remaining into validation and test sets without shuffling\n",
    "val, test = train_test_split(remaining, test_size=0.5, shuffle=False, random_state=42)\n",
    "\n",
    "# Save splits to CSV\n",
    "train.to_csv(\"dataset/train_reviews.csv\", index=False)\n",
    "val.to_csv(\"dataset/val_reviews.csv\", index=False)\n",
    "test.to_csv(\"dataset/test_reviews.csv\", index=False)\n",
    "\n",
    "# Output sizes\n",
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Validation set size: {len(val)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274d8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Training models with optimized parameters...\n",
      "\n",
      "============================================================\n",
      "           SIMILAR WORDS ANALYSIS (PDF-COMPLIANT)           \n",
      "============================================================\n",
      "\n",
      "---------------------------TASTY----------------------------\n",
      "Word2Vec                       | FastText                      \n",
      "------------------------------------------------------------\n",
      "yummy                          | tasteful                      \n",
      "good                           | flavorful                     \n",
      "delicious                      | delicious                     \n",
      "delish                         | yummy                         \n",
      "flavorful                      | good                          \n",
      "filling                        | goodsized                     \n",
      "satisfying                     | delicioso                     \n",
      "disappointing                  | delicacy                      \n",
      "hearty                         | flavourful                    \n",
      "bland                          | delic                         \n",
      "plentiful                      | nasty                         \n",
      "oily                           | goodbye                       \n",
      "decent                         | goodbut                       \n",
      "delectable                     | goodthe                       \n",
      "strong                         | yumm                          \n",
      "\n",
      "----------------------------GIVE----------------------------\n",
      "Word2Vec                       | FastText                      \n",
      "------------------------------------------------------------\n",
      "rate                           | forgive                       \n",
      "consider                       | agave                         \n",
      "giving                         | gives                         \n",
      "allow                          | given                         \n",
      "receive                        | gave                          \n",
      "tell                           | ave                           \n",
      "add                            | dave                          \n",
      "deserve                        | suave                         \n",
      "suggest                        | five                          \n",
      "bring                          | brave                         \n",
      "help                           | receive                       \n",
      "get                            | save                          \n",
      "gave                           | wave                          \n",
      "convince                       | giving                        \n",
      "take                           | nerve                         \n",
      "\n",
      "Models saved to 'models' directory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import os\n",
    "import string\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "SEED = 1234  \n",
    "\n",
    "# ==================== TEXT PREPROCESSING (PDF-COMPLIANT) ====================\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    PDF-compliant preprocessing (Page 4):\n",
    "    1. Tokenization using NLTK\n",
    "    2. Lowercasing\n",
    "    Note: No stopword removal/lemmatization mentioned in PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PDF explicitly mentions lowercase conversion\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        \n",
    "        # Remove punctuation (implied by \"tokenization\" in NLP standards)\n",
    "        tokens = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens]\n",
    "        \n",
    "        # Remove empty strings\n",
    "        return [word for word in tokens if word]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# ==================== MODEL TRAINING (PDF-COMPLIANT IMPROVEMENTS) ====================\n",
    "def train_models(texts):\n",
    "    \"\"\"\n",
    "    Train models with PDF-allowed optimizations:\n",
    "    - Uses only SGNS (as implied by PDF)\n",
    "    - Adjusts vector_size/window/min_count within gensim defaults\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining models with optimized parameters...\")\n",
    "    \n",
    "    # Word2Vec with improved parameters (still within gensim defaults)\n",
    "    w2v = Word2Vec(\n",
    "        sentences=texts,\n",
    "        vector_size=200,  # Default:100 (PDF doesn't restrict this)\n",
    "        window=8,         # Default:5 (PDF doesn't restrict)\n",
    "        min_count=8,      # Default:5 (PDF doesn't restrict)\n",
    "        workers=4,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    # FastText with subword optimization\n",
    "    ft = FastText(\n",
    "        sentences=texts,\n",
    "        vector_size=200,\n",
    "        window=8,\n",
    "        min_count=8,\n",
    "        workers=4,\n",
    "        seed=SEED,\n",
    "        min_n=3,  # PDF doesn't restrict subword settings\n",
    "        max_n=6\n",
    "    )\n",
    "    \n",
    "    return w2v, ft\n",
    "\n",
    "# ==================== SIMILAR WORDS ANALYSIS ====================\n",
    "def get_high_quality_similar(model, word, topn=15):\n",
    "    \"\"\"\n",
    "    Get similar words with quality filtering (post-processing)\n",
    "    PDF-compliant as it doesn't modify training process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get more samples then filter (PDF only asks for top 15, doesn't restrict how)\n",
    "        similar = model.wv.most_similar(word, topn=30)\n",
    "        \n",
    "        # Filter by similarity score (post-processing allowed)\n",
    "        filtered = [word for word, score in similar if score > 0.3][:topn]\n",
    "        \n",
    "        return filtered or [\"<OOV>\"] * topn\n",
    "    except KeyError:\n",
    "        return [\"<OOV>\"] * topn  # As mentioned in PDF page 5\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading training data...\")\n",
    "    train_reviews = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "    train_texts = train_reviews['text'].fillna(\"\").apply(preprocess_text).tolist()\n",
    "    train_texts = [t for t in train_texts if t]  # Remove empty\n",
    "    \n",
    "    # Train models\n",
    "    w2v_model, ft_model = train_models(train_texts)\n",
    "    \n",
    "    # Analyze words\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"{:^60}\".format(\" SIMILAR WORDS ANALYSIS (PDF-COMPLIANT) \"))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for word in [\"tasty\", \"give\"]:\n",
    "        print(f\"\\n{word.upper():-^60}\")\n",
    "        print(f\"{'Word2Vec':<30} | {'FastText':<30}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        w2v_sim = get_high_quality_similar(w2v_model, word)\n",
    "        ft_sim = get_high_quality_similar(ft_model, word)\n",
    "        \n",
    "        for i in range(15):\n",
    "            print(f\"{w2v_sim[i]:<30} | {ft_sim[i]:<30}\")\n",
    "    \n",
    "    # Save models (optional)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    w2v_model.save(\"models/word2vec_optimized.model\")\n",
    "    ft_model.save(\"models/fasttext_optimized.model\")\n",
    "    print(\"\\nModels saved to 'models' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b76d5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.1: Implementing the Recommender System...\n"
     ]
    }
   ],
   "source": [
    "# Cell for Section 3.1: Import Libraries and Initial Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# Download NLTK data (optional if already downloaded in Section 2.1)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.1: Implementing the Recommender System...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c803b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 20000\n",
      "Validation set size: 1962\n",
      "Test set size: 1962\n"
     ]
    }
   ],
   "source": [
    "# Cell for Section 3.1: Load and Split Data (Using Section 1.2 Output)\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "test_df = pd.read_csv(\"dataset/test_reviews.csv\")\n",
    "\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "val_df['date'] = pd.to_datetime(val_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1653aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Section 3.1: Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['tokens'] = train_df['text'].apply(preprocess_text)\n",
    "val_df['tokens'] = val_df['text'].apply(preprocess_text)\n",
    "test_df['tokens'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Convert list of tokens to string so it can be saved in CSV\n",
    "train_df['tokens_str'] = train_df['tokens'].apply(str)\n",
    "val_df['tokens_str'] = val_df['tokens'].apply(str)\n",
    "test_df['tokens_str'] = test_df['tokens'].apply(str)\n",
    "\n",
    "# Save processed datasets with token strings\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "train_df.to_csv(\"dataset/train_reviews.csv\", index=False)\n",
    "val_df.to_csv(\"dataset/val_reviews.csv\", index=False)\n",
    "test_df.to_csv(\"dataset/test_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222af7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Section 3.1: Learn Embeddings and Create Document Embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode='sg', vector_size=100, window=5, epochs=5):\n",
    "    if embedding_type == 'word2vec':\n",
    "        model = Word2Vec(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=1,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234\n",
    "        )\n",
    "    elif embedding_type == 'fasttext':\n",
    "        model = FastText(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=1,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234\n",
    "        )\n",
    "    elif embedding_type == 'doc2vec':\n",
    "        model = None  # Doc2Vec is handled separately\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding type\")\n",
    "    return model\n",
    "\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    if embedding_type == 'doc2vec':\n",
    "        return model.infer_vector(tokens)\n",
    "    else:\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if embedding_type == 'word2vec' and token not in model.wv and handle_oov == 'ignore':\n",
    "                continue\n",
    "            vectors.append(model.wv[token])\n",
    "        if not vectors:\n",
    "            return np.zeros(model.vector_size)\n",
    "        if agg_type == 'average':\n",
    "            return np.mean(vectors, axis=0)\n",
    "        elif agg_type == 'sum':\n",
    "            return np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9cef097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Section 3.1: Feature Manipulation and Regression Setup\n",
    "# Step 4: Feature Manipulation (Normalization)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 5: Regression Algorithms\n",
    "regressors = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=1234),\n",
    "    'HistGradientBoosting': HistGradientBoostingRegressor(random_state=1234)\n",
    "}\n",
    "\n",
    "# Experiment with different configurations\n",
    "embedding_types = ['word2vec', 'fasttext', 'doc2vec']\n",
    "modes = ['sg', 'cbow']  # Test both SGNS and CBOW\n",
    "agg_types = ['average', 'sum', 'doc2vec']\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea3ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with word2vec...\n",
      "  Mode: sg\n",
      "    Aggregation: average\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "    Aggregation: sum\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "  Mode: cbow\n",
      "    Aggregation: average\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "    Aggregation: sum\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "\n",
      "Training with fasttext...\n",
      "  Mode: sg\n",
      "    Aggregation: average\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "    Aggregation: sum\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "  Mode: cbow\n",
      "    Aggregation: average\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "    Aggregation: sum\n",
      "      Evaluating LinearRegression...\n",
      "      Evaluating Ridge...\n",
      "      Evaluating RandomForest...\n",
      "      Evaluating HistGradientBoosting...\n",
      "\n",
      "Training with doc2vec...\n",
      "  Mode: dm\n",
      "    Evaluating LinearRegression...\n",
      "    Evaluating Ridge...\n",
      "    Evaluating RandomForest...\n",
      "    Evaluating HistGradientBoosting...\n",
      "  Mode: dbow\n",
      "    Evaluating LinearRegression...\n",
      "    Evaluating Ridge...\n",
      "    Evaluating RandomForest...\n",
      "    Evaluating HistGradientBoosting...\n",
      "\n",
      "Regression Results on Validation Set:\n",
      "   embedding_type  mode agg_type             regressor  r2_score\n",
      "0        word2vec    sg  average      LinearRegression  0.331493\n",
      "1        word2vec    sg  average                 Ridge  0.331494\n",
      "2        word2vec    sg  average          RandomForest  0.307856\n",
      "3        word2vec    sg  average  HistGradientBoosting  0.376162\n",
      "4        word2vec    sg      sum      LinearRegression  0.295532\n",
      "5        word2vec    sg      sum                 Ridge  0.295591\n",
      "6        word2vec    sg      sum          RandomForest  0.267283\n",
      "7        word2vec    sg      sum  HistGradientBoosting  0.304005\n",
      "8        word2vec  cbow  average      LinearRegression  0.301270\n",
      "9        word2vec  cbow  average                 Ridge  0.301270\n",
      "10       word2vec  cbow  average          RandomForest  0.271438\n",
      "11       word2vec  cbow  average  HistGradientBoosting  0.339193\n",
      "12       word2vec  cbow      sum      LinearRegression  0.272667\n",
      "13       word2vec  cbow      sum                 Ridge  0.272665\n",
      "14       word2vec  cbow      sum          RandomForest  0.250354\n",
      "15       word2vec  cbow      sum  HistGradientBoosting  0.303889\n",
      "16       fasttext    sg  average      LinearRegression  0.334131\n",
      "17       fasttext    sg  average                 Ridge  0.334134\n",
      "18       fasttext    sg  average          RandomForest  0.338209\n",
      "19       fasttext    sg  average  HistGradientBoosting  0.387499\n",
      "20       fasttext    sg      sum      LinearRegression  0.294009\n",
      "21       fasttext    sg      sum                 Ridge  0.294075\n",
      "22       fasttext    sg      sum          RandomForest  0.261530\n",
      "23       fasttext    sg      sum  HistGradientBoosting  0.320436\n",
      "24       fasttext  cbow  average      LinearRegression  0.284155\n",
      "25       fasttext  cbow  average                 Ridge  0.284153\n",
      "26       fasttext  cbow  average          RandomForest  0.208165\n",
      "27       fasttext  cbow  average  HistGradientBoosting  0.282841\n",
      "28       fasttext  cbow      sum      LinearRegression  0.259346\n",
      "29       fasttext  cbow      sum                 Ridge  0.259334\n",
      "30       fasttext  cbow      sum          RandomForest  0.190701\n",
      "31       fasttext  cbow      sum  HistGradientBoosting  0.240752\n",
      "32        doc2vec    dm  doc2vec      LinearRegression  0.238849\n",
      "33        doc2vec    dm  doc2vec                 Ridge  0.238850\n",
      "34        doc2vec    dm  doc2vec          RandomForest  0.167363\n",
      "35        doc2vec    dm  doc2vec  HistGradientBoosting  0.241162\n",
      "36        doc2vec  dbow  doc2vec      LinearRegression  0.296252\n",
      "37        doc2vec  dbow  doc2vec                 Ridge  0.296252\n",
      "38        doc2vec  dbow  doc2vec          RandomForest  0.271995\n",
      "39        doc2vec  dbow  doc2vec  HistGradientBoosting  0.311801\n",
      "\n",
      "Best Regressor: HistGradientBoosting\n",
      "Embedding Type: fasttext\n",
      "Mode: sg\n",
      "Aggregation Type: average\n",
      "R2 Score: 0.3875\n",
      "Rationale: The HistGradientBoosting with fasttext embeddings, sg mode, and average aggregation achieved the highest R2 score on the validation set, indicating the best fit for predicting restaurant ratings based on textual reviews.\n"
     ]
    }
   ],
   "source": [
    "# Cell for Section 3.1: Experiment and Report Results\n",
    "for embedding_type in embedding_types:\n",
    "    print(f\"\\nTraining with {embedding_type}...\")\n",
    "    \n",
    "    if embedding_type == 'doc2vec':\n",
    "        # Train Doc2Vec with meaningful tags\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(\n",
    "                words=tokens,\n",
    "                tags=[f\"{row['user_id']}_{row['business_id']}\"]\n",
    "            ) for _, row in train_df.iterrows() for tokens in [row['tokens']]\n",
    "        ]\n",
    "        for mode in ['dm', 'dbow']:  # Test both DM and DBOW\n",
    "            print(f\"  Mode: {mode}\")\n",
    "            doc2vec_model = Doc2Vec(\n",
    "                tagged_docs,\n",
    "                vector_size=100,\n",
    "                window=5,\n",
    "                min_count=1,\n",
    "                dm=1 if mode == 'dm' else 0,\n",
    "                epochs=5,\n",
    "                seed=1234\n",
    "            )\n",
    "            \n",
    "            # Create document embeddings\n",
    "            train_features = np.array([doc2vec_model.infer_vector(tokens) for tokens in train_df['tokens']])\n",
    "            val_features = np.array([doc2vec_model.infer_vector(tokens) for tokens in val_df['tokens']])\n",
    "            \n",
    "            # Normalize features\n",
    "            train_features = scaler.fit_transform(train_features)\n",
    "            val_features = scaler.transform(val_features)\n",
    "            \n",
    "            # Train and evaluate regressors\n",
    "            for reg_name, regressor in regressors.items():\n",
    "                print(f\"    Evaluating {reg_name}...\")\n",
    "                regressor.fit(train_features, train_df['stars'])\n",
    "                val_pred = regressor.predict(val_features)\n",
    "                r2 = r2_score(val_df['stars'], val_pred)\n",
    "                results.append({\n",
    "                    'embedding_type': embedding_type,\n",
    "                    'mode': mode,\n",
    "                    'agg_type': 'doc2vec',\n",
    "                    'regressor': reg_name,\n",
    "                    'r2_score': r2\n",
    "                })\n",
    "    else:\n",
    "        for mode in modes:\n",
    "            print(f\"  Mode: {mode}\")\n",
    "            # Train Word2Vec or FastText\n",
    "            model = train_embeddings(train_df['tokens'], embedding_type, mode=mode)\n",
    "            \n",
    "            for agg_type in ['average', 'sum']:\n",
    "                print(f\"    Aggregation: {agg_type}\")\n",
    "                # Create document embeddings\n",
    "                train_features = np.array([\n",
    "                    get_doc_embedding(tokens, model, embedding_type, agg_type) \n",
    "                    for tokens in train_df['tokens']\n",
    "                ])\n",
    "                val_features = np.array([\n",
    "                    get_doc_embedding(tokens, model, embedding_type, agg_type) \n",
    "                    for tokens in val_df['tokens']\n",
    "                ])\n",
    "                \n",
    "                # Normalize features\n",
    "                train_features = scaler.fit_transform(train_features)\n",
    "                val_features = scaler.transform(val_features)\n",
    "                \n",
    "                # Train and evaluate regressors\n",
    "                for reg_name, regressor in regressors.items():\n",
    "                    print(f\"      Evaluating {reg_name}...\")\n",
    "                    regressor.fit(train_features, train_df['stars'])\n",
    "                    val_pred = regressor.predict(val_features)\n",
    "                    r2 = r2_score(val_df['stars'], val_pred)\n",
    "                    results.append({\n",
    "                        'embedding_type': embedding_type,\n",
    "                        'mode': mode,\n",
    "                        'agg_type': agg_type,\n",
    "                        'regressor': reg_name,\n",
    "                        'r2_score': r2\n",
    "                    })\n",
    "\n",
    "# Report results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nRegression Results on Validation Set:\")\n",
    "print(results_df)\n",
    "\n",
    "# Select best regressor\n",
    "best_result = results_df.loc[results_df['r2_score'].idxmax()]\n",
    "print(f\"\\nBest Regressor: {best_result['regressor']}\")\n",
    "print(f\"Embedding Type: {best_result['embedding_type']}\")\n",
    "print(f\"Mode: {best_result['mode']}\")\n",
    "print(f\"Aggregation Type: {best_result['agg_type']}\")\n",
    "print(f\"R2 Score: {best_result['r2_score']:.4f}\")\n",
    "print(f\"Rationale: The {best_result['regressor']} with {best_result['embedding_type']} embeddings, {best_result['mode']} mode, and {best_result['agg_type']} aggregation achieved the highest R2 score on the validation set, indicating the best fit for predicting restaurant ratings based on textual reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91ab12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.2: Hyperparameter Tuning with Optuna for word2vec-SGNS\n",
      "\n",
      "Optimizing WORD2VEC - SG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.426653: 100%|██████████| 20/20 [29:36<00:00, 88.80s/it]  \n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_8892\\1478852570.py:154: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna_viz.plot_param_importances(study)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Optimization Results for word2vec-SGNS:\n",
      "    setting  best_r2  vector_size  window  epochs  min_count agg_type  learning_rate  max_iter  max_depth\n",
      "word2vec-sg 0.426653          150      10      10          5  average       0.032309       300          6\n",
      "\n",
      "Results saved to 'dataset/hyperparameter_results_word2vec_sg.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as optuna_viz\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.2: Hyperparameter Tuning with Optuna for word2vec-SGNS\")\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'word2vec':\n",
    "        model = Word2Vec(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            workers=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if embedding_type == 'word2vec' and token not in model.wv and handle_oov == 'ignore':\n",
    "            continue\n",
    "        vectors.append(model.wv[token])\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0) if agg_type == 'average' else np.sum(vectors, axis=0)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, embedding_type, mode, train_df, val_df):\n",
    "    params = {\n",
    "        'vector_size': trial.suggest_int('vector_size', 50, 150, step=50),\n",
    "        'window': trial.suggest_int('window', 3, 10),\n",
    "        'epochs': trial.suggest_int('epochs', 5, 10, step=5),\n",
    "        'min_count': trial.suggest_int('min_count', 1, 10),\n",
    "        'agg_type': trial.suggest_categorical('agg_type', ['average', 'sum']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 300, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "\n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "\n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "\n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "\n",
    "    # Predict and evaluate\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    return r2_score(val_df['stars'], val_pred)\n",
    "\n",
    "# Settings\n",
    "embedding_type, mode = 'word2vec', 'sg'\n",
    "n_trials = 20\n",
    "n_jobs = 8\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nOptimizing {embedding_type.upper()} - {mode.upper()}...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    study_name=f\"{embedding_type}_{mode}\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    partial(objective, embedding_type=embedding_type, mode=mode, train_df=train_df, val_df=val_df),\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=n_jobs,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "results.append({\n",
    "    'setting': f\"{embedding_type}-{mode}\",\n",
    "    'best_r2': study.best_value,\n",
    "    **best_params\n",
    "})\n",
    "\n",
    "# Plot hyperparameter importance using matplotlib\n",
    "optuna_viz.plot_param_importances(study)\n",
    "plt.title(f\"Hyperparameter Importance - {embedding_type}-{mode}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/param_importance_{embedding_type}_{mode}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Optimization Results for word2vec-SGNS:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(f\"dataset/hyperparameter_results_{embedding_type}_{mode}.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'dataset/hyperparameter_results_{embedding_type}_{mode}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55eeda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.2: Hyperparameter Tuning with Optuna for word2vec-CBOW\n",
      "\n",
      "Optimizing WORD2VEC - CBOW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 44. Best value: 0.416508: 100%|██████████| 50/50 [1:15:25<00:00, 90.50s/it]  \n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_10024\\1590787207.py:154: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna_viz.plot_param_importances(study)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Optimization Results for word2vec-CBOW:\n",
      "      setting  best_r2  vector_size  window  epochs  min_count agg_type  learning_rate  max_iter  max_depth\n",
      "word2vec-cbow 0.416508          150       8      10          3  average       0.088171       200          8\n",
      "\n",
      "Results saved to 'dataset/hyperparameter_results_word2vec_cbow.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as optuna_viz\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.2: Hyperparameter Tuning with Optuna for word2vec-CBOW\")\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'word2vec':\n",
    "        model = Word2Vec(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            workers=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if embedding_type == 'word2vec' and token not in model.wv and handle_oov == 'ignore':\n",
    "            continue\n",
    "        vectors.append(model.wv[token])\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0) if agg_type == 'average' else np.sum(vectors, axis=0)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, embedding_type, mode, train_df, val_df):\n",
    "    params = {\n",
    "        'vector_size': trial.suggest_int('vector_size', 50, 150, step=50),\n",
    "        'window': trial.suggest_int('window', 3, 10),\n",
    "        'epochs': trial.suggest_int('epochs', 5, 10, step=5),\n",
    "        'min_count': trial.suggest_int('min_count', 1, 10),\n",
    "        'agg_type': trial.suggest_categorical('agg_type', ['average', 'sum']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 300, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "\n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "\n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "\n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "\n",
    "    # Predict and evaluate\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    return r2_score(val_df['stars'], val_pred)\n",
    "\n",
    "# Settings\n",
    "embedding_type, mode = 'word2vec', 'cbow'\n",
    "n_trials = 50\n",
    "n_jobs = 8\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nOptimizing {embedding_type.upper()} - {mode.upper()}...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    study_name=f\"{embedding_type}_{mode}\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    partial(objective, embedding_type=embedding_type, mode=mode, train_df=train_df, val_df=val_df),\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=n_jobs,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "results.append({\n",
    "    'setting': f\"{embedding_type}-{mode}\",\n",
    "    'best_r2': study.best_value,\n",
    "    **best_params\n",
    "})\n",
    "\n",
    "# Plot hyperparameter importance using matplotlib\n",
    "optuna_viz.plot_param_importances(study)\n",
    "plt.title(f\"Hyperparameter Importance - {embedding_type}-{mode}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/param_importance_{embedding_type}_{mode}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Optimization Results for word2vec-CBOW:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(f\"dataset/hyperparameter_results_{embedding_type}_{mode}.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'dataset/hyperparameter_results_{embedding_type}_{mode}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2baf8184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.2: Hyperparameter Tuning with Optuna for fastText-SGNS\n",
      "\n",
      "Optimizing FASTTEXT - SG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.427574: 100%|██████████| 15/15 [1:39:42<00:00, 398.81s/it]  \n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_13792\\2116729553.py:154: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna_viz.plot_param_importances(study)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Optimization Results for fastText-SGNS:\n",
      "    setting  best_r2  vector_size  window  epochs  min_count agg_type  min_n  max_n  learning_rate  max_iter  max_depth\n",
      "fasttext-sg 0.427574          100      10      10          6  average      4      7       0.083989       200          7\n",
      "\n",
      "Results saved to 'dataset/hyperparameter_results_fasttext_sg.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as optuna_viz\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.2: Hyperparameter Tuning with Optuna for fastText-SGNS\")\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'fasttext':\n",
    "        model = FastText(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            min_n=params['min_n'],\n",
    "            max_n=params['max_n'],\n",
    "            workers=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    vectors = [model.wv[token] for token in tokens]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0) if agg_type == 'average' else np.sum(vectors, axis=0)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, embedding_type, mode, train_df, val_df):\n",
    "    params = {\n",
    "        'vector_size': trial.suggest_int('vector_size', 50, 150, step=50),\n",
    "        'window': trial.suggest_int('window', 3, 10),\n",
    "        'epochs': trial.suggest_int('epochs', 5, 10, step=5),\n",
    "        'min_count': trial.suggest_int('min_count', 1, 10),\n",
    "        'agg_type': trial.suggest_categorical('agg_type', ['average', 'sum']),\n",
    "        'min_n': trial.suggest_int('min_n', 2, 4),\n",
    "        'max_n': trial.suggest_int('max_n', 5, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 300, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "\n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "\n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "\n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "\n",
    "    # Predict and evaluate\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    return r2_score(val_df['stars'], val_pred)\n",
    "\n",
    "# Settings\n",
    "embedding_type, mode = 'fasttext', 'sg'\n",
    "n_trials = 15\n",
    "n_jobs = 8\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nOptimizing {embedding_type.upper()} - {mode.upper()}...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    study_name=f\"{embedding_type}_{mode}\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    partial(objective, embedding_type=embedding_type, mode=mode, train_df=train_df, val_df=val_df),\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=n_jobs,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "results.append({\n",
    "    'setting': f\"{embedding_type}-{mode}\",\n",
    "    'best_r2': study.best_value,\n",
    "    **best_params\n",
    "})\n",
    "\n",
    "# Plot hyperparameter importance using matplotlib\n",
    "optuna_viz.plot_param_importances(study)\n",
    "plt.title(f\"Hyperparameter Importance - {embedding_type}-{mode}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/param_importance_{embedding_type}_{mode}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Optimization Results for fastText-SGNS:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(f\"dataset/hyperparameter_results_{embedding_type}_{mode}.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'dataset/hyperparameter_results_{embedding_type}_{mode}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f6903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.2: Hyperparameter Tuning with Optuna for fastText-CBOW\n",
      "\n",
      "Optimizing FASTTEXT - CBOW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.12.3)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as optuna_viz\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.2: Hyperparameter Tuning with Optuna for fastText-CBOW\")\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'fasttext':\n",
    "        model = FastText(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            min_n=params['min_n'],\n",
    "            max_n=params['max_n'],\n",
    "            workers=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    vectors = [model.wv[token] for token in tokens]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0) if agg_type == 'average' else np.sum(vectors, axis=0)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, embedding_type, mode, train_df, val_df):\n",
    "    params = {\n",
    "        'vector_size': trial.suggest_int('vector_size', 50, 150, step=50),\n",
    "        'window': trial.suggest_int('window', 3, 10),\n",
    "        'epochs': trial.suggest_int('epochs', 5, 10, step=5),\n",
    "        'min_count': trial.suggest_int('min_count', 1, 10),\n",
    "        'agg_type': trial.suggest_categorical('agg_type', ['average', 'sum']),\n",
    "        'min_n': trial.suggest_int('min_n', 2, 4),\n",
    "        'max_n': trial.suggest_int('max_n', 5, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 300, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "\n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "\n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "\n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "\n",
    "    # Predict and evaluate\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    return r2_score(val_df['stars'], val_pred)\n",
    "\n",
    "# Settings\n",
    "embedding_type, mode = 'fasttext', 'cbow'\n",
    "n_trials = 30\n",
    "n_jobs = 10\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nOptimizing {embedding_type.upper()} - {mode.upper()}...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    study_name=f\"{embedding_type}_{mode}\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    partial(objective, embedding_type=embedding_type, mode=mode, train_df=train_df, val_df=val_df),\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=n_jobs,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "results.append({\n",
    "    'setting': f\"{embedding_type}-{mode}\",\n",
    "    'best_r2': study.best_value,\n",
    "    **best_params\n",
    "})\n",
    "\n",
    "# Plot hyperparameter importance using matplotlib\n",
    "optuna_viz.plot_param_importances(study)\n",
    "plt.title(f\"Hyperparameter Importance - {embedding_type}-{mode}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/param_importance_{embedding_type}_{mode}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Optimization Results for fastText-CBOW:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(f\"dataset/hyperparameter_results_{embedding_type}_{mode}.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'dataset/hyperparameter_results_{embedding_type}_{mode}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c614985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.2: Hyperparameter Tuning with Optuna for doc2vec-DM\n",
      "\n",
      "Optimizing DOC2VEC - DM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: 0.349287: 100%|██████████| 50/50 [2:10:02<00:00, 156.05s/it]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_10024\\593085586.py:154: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna_viz.plot_param_importances(study)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Optimization Results for doc2vec-DM:\n",
      "   setting  best_r2  vector_size  window  epochs  min_count  alpha  learning_rate  max_iter  max_depth\n",
      "doc2vec-dm 0.349287          100       3      10          8   0.05        0.06995       200         10\n",
      "\n",
      "Results saved to 'dataset/hyperparameter_results_doc2vec_dm.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as optuna_viz\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.2: Hyperparameter Tuning with Optuna for doc2vec-DM\")\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'doc2vec':\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(words=tokens, tags=[f\"{row['user_id']}_{row['business_id']}\"])\n",
    "            for _, row in train_df.iterrows() for tokens in [row['tokens']]\n",
    "        ]\n",
    "        model = Doc2Vec(\n",
    "            tagged_docs,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            dm=1 if mode == 'dm' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            alpha=params['alpha'],\n",
    "            workers=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    return model.infer_vector(tokens)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, embedding_type, mode, train_df, val_df):\n",
    "    params = {\n",
    "        'vector_size': trial.suggest_int('vector_size', 50, 150, step=50),\n",
    "        'window': trial.suggest_int('window', 3, 10),\n",
    "        'epochs': trial.suggest_int('epochs', 5, 10, step=5),\n",
    "        'min_count': trial.suggest_int('min_count', 1, 10),\n",
    "        'alpha': trial.suggest_float('alpha', 0.01, 0.05, step=0.01),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 300, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "    params['agg_type'] = 'doc2vec'\n",
    "\n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "\n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "\n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "\n",
    "    # Predict and evaluate\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    return r2_score(val_df['stars'], val_pred)\n",
    "\n",
    "# Settings\n",
    "embedding_type, mode = 'doc2vec', 'dm'\n",
    "n_trials = 50\n",
    "n_jobs = 8\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nOptimizing {embedding_type.upper()} - {mode.upper()}...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    study_name=f\"{embedding_type}_{mode}\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    partial(objective, embedding_type=embedding_type, mode=mode, train_df=train_df, val_df=val_df),\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=n_jobs,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "results.append({\n",
    "    'setting': f\"{embedding_type}-{mode}\",\n",
    "    'best_r2': study.best_value,\n",
    "    **best_params\n",
    "})\n",
    "\n",
    "# Plot hyperparameter importance using matplotlib\n",
    "optuna_viz.plot_param_importances(study)\n",
    "plt.title(f\"Hyperparameter Importance - {embedding_type}-{mode}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/param_importance_{embedding_type}_{mode}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Optimization Results for doc2vec-DM:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(f\"dataset/hyperparameter_results_{embedding_type}_{mode}.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'dataset/hyperparameter_results_{embedding_type}_{mode}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7362179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3.2: Hyperparameter Tuning with Optuna for doc2vec-DBOW\n",
      "\n",
      "Optimizing DOC2VEC - DBOW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.424175: 100%|██████████| 30/30 [27:54<00:00, 55.81s/it] \n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_9944\\2900203671.py:154: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna_viz.plot_param_importances(study)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Optimization Results for doc2vec-DBOW:\n",
      "     setting  best_r2  vector_size  window  epochs  min_count  alpha  learning_rate  max_iter  max_depth\n",
      "doc2vec-dbow 0.424175           50       5      10          2   0.03       0.059124       200          6\n",
      "\n",
      "Results saved to 'dataset/hyperparameter_results_doc2vec_dbow.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as optuna_viz\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Section 3.2: Hyperparameter Tuning with Optuna for doc2vec-DBOW\")\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'doc2vec':\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(words=tokens, tags=[f\"{row['user_id']}_{row['business_id']}\"])\n",
    "            for _, row in train_df.iterrows() for tokens in [row['tokens']]\n",
    "        ]\n",
    "        model = Doc2Vec(\n",
    "            tagged_docs,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            dm=1 if mode == 'dm' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            alpha=params['alpha'],\n",
    "            workers=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type, handle_oov='ignore'):\n",
    "    return model.infer_vector(tokens)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial, embedding_type, mode, train_df, val_df):\n",
    "    params = {\n",
    "        'vector_size': trial.suggest_int('vector_size', 50, 150, step=50),\n",
    "        'window': trial.suggest_int('window', 3, 10),\n",
    "        'epochs': trial.suggest_int('epochs', 5, 10, step=5),\n",
    "        'min_count': trial.suggest_int('min_count', 1, 10),\n",
    "        'alpha': trial.suggest_float('alpha', 0.01, 0.05, step=0.01),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 300, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "    }\n",
    "    params['agg_type'] = 'doc2vec'\n",
    "\n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "\n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "\n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "\n",
    "    # Predict and evaluate\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    return r2_score(val_df['stars'], val_pred)\n",
    "\n",
    "# Settings\n",
    "embedding_type, mode = 'doc2vec', 'dbow'\n",
    "n_trials = 30\n",
    "n_jobs = 8\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nOptimizing {embedding_type.upper()} - {mode.upper()}...\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    study_name=f\"{embedding_type}_{mode}\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    partial(objective, embedding_type=embedding_type, mode=mode, train_df=train_df, val_df=val_df),\n",
    "    n_trials=n_trials,\n",
    "    n_jobs=n_jobs,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "results.append({\n",
    "    'setting': f\"{embedding_type}-{mode}\",\n",
    "    'best_r2': study.best_value,\n",
    "    **best_params\n",
    "})\n",
    "\n",
    "# Plot hyperparameter importance using matplotlib\n",
    "optuna_viz.plot_param_importances(study)\n",
    "plt.title(f\"Hyperparameter Importance - {embedding_type}-{mode}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/param_importance_{embedding_type}_{mode}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nHyperparameter Optimization Results for doc2vec-DBOW:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(f\"dataset/hyperparameter_results_{embedding_type}_{mode}.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'dataset/hyperparameter_results_{embedding_type}_{mode}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb4c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Hyperparameter Optimization Results:\n",
      "      setting  best_r2  vector_size  window  epochs  min_count  alpha  learning_rate  max_iter  max_depth agg_type  min_n  max_n\n",
      " doc2vec-dbow     0.42           50       5      10          2   0.03       0.059124       200          6      NaN    NaN    NaN\n",
      "   doc2vec-dm     0.35          100       3      10          8   0.05       0.069950       200         10      NaN    NaN    NaN\n",
      "fasttext-cbow     0.30          150      10       5          4    NaN       0.171080       200          7  average    2.0    7.0\n",
      "  fasttext-sg     0.43          100      10      10          6    NaN       0.083989       200          7  average    4.0    7.0\n",
      "word2vec-cbow     0.42          150       8      10          3    NaN       0.088171       200          8  average    NaN    NaN\n",
      "  word2vec-sg     0.43          150      10      10          5    NaN       0.032309       300          6  average    NaN    NaN\n",
      "\n",
      "Combined results saved to 'dataset\\hyperparameter_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find all hyperparameter result CSV files\n",
    "result_files = glob.glob(os.path.join(output_dir, \"hyperparameter_results_*.csv\"))\n",
    "\n",
    "# Check if any files were found\n",
    "if not result_files:\n",
    "    raise FileNotFoundError(\"No hyperparameter result CSV files found in 'dataset' directory. Please ensure the 6 codes for section 3.2 have been run.\")\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "results_df = pd.concat([pd.read_csv(f) for f in result_files], ignore_index=True, sort=False)\n",
    "\n",
    "# Round best_r2 to 2 decimal places\n",
    "results_df['best_r2'] = results_df['best_r2'].round(2)\n",
    "\n",
    "# Sort by setting for better readability (optional)\n",
    "results_df = results_df.sort_values(by='setting')\n",
    "\n",
    "# Save combined results to a single CSV file\n",
    "output_path = os.path.join(output_dir, \"hyperparameter_results.csv\")\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print combined results\n",
    "print(\"\\nCombined Hyperparameter Optimization Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nCombined results saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ddaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "metric                CCC  Kendall   MAE  MAPE  NDCG@10  NDCG@20  NDCG@50  Pearson    R2  RMSE  Spearman\n",
      "setting       split                                                                                     \n",
      "doc2vec-dbow  test   0.58     0.49  0.57  0.18     0.97     0.99     0.00     0.67  0.43  0.73      0.61\n",
      "              train  0.72     0.62  0.48  0.15     0.98     0.98     0.98     0.79  0.60  0.60      0.75\n",
      "              val    0.57     0.51  0.58  0.19     0.96     0.94     0.00     0.65  0.41  0.74      0.63\n",
      "doc2vec-dm    test   0.54     0.45  0.61  0.19     0.97     0.98     0.00     0.63  0.37  0.76      0.56\n",
      "              train  0.76     0.65  0.45  0.14     0.98     0.98     0.99     0.83  0.65  0.56      0.78\n",
      "              val    0.53     0.47  0.61  0.20     0.96     0.89     0.00     0.60  0.36  0.78      0.59\n",
      "fasttext-cbow test   0.48     0.42  0.64  0.20     0.97     0.99     0.00     0.58  0.31  0.80      0.53\n",
      "              train  0.65     0.58  0.52  0.16     0.97     0.98     0.98     0.75  0.53  0.65      0.71\n",
      "              val    0.44     0.41  0.65  0.21     0.95     0.93     0.00     0.53  0.27  0.82      0.52\n",
      "fasttext-sg   test   0.60     0.50  0.57  0.18     0.97     0.99     0.00     0.69  0.45  0.71      0.61\n",
      "              train  0.75     0.63  0.46  0.14     0.98     0.98     0.98     0.80  0.63  0.58      0.76\n",
      "              val    0.58     0.49  0.58  0.19     0.97     0.92     0.00     0.65  0.42  0.74      0.61\n",
      "word2vec-cbow test   0.55     0.47  0.60  0.19     0.97     0.99     0.00     0.64  0.39  0.75      0.58\n",
      "              train  0.75     0.64  0.46  0.14     0.98     0.98     0.99     0.82  0.64  0.57      0.77\n",
      "              val    0.55     0.48  0.59  0.19     0.97     0.94     0.00     0.63  0.39  0.76      0.61\n",
      "word2vec-sg   test   0.61     0.51  0.56  0.17     0.98     0.99     0.00     0.70  0.47  0.70      0.63\n",
      "              train  0.74     0.64  0.46  0.14     0.98     0.98     0.98     0.81  0.63  0.58      0.77\n",
      "              val    0.57     0.50  0.58  0.19     0.96     0.92     0.00     0.65  0.42  0.74      0.62\n",
      "\n",
      "Results saved to 'dataset/evaluation_results.csv'\n",
      "Plots saved to 'figures/r2_bar_plot.png' and 'figures/ndcg_line_plot.png'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import ndcg_score\n",
    "import multiprocessing\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Create figures directory\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "# Load data from Section 1\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "test_df = pd.read_csv(\"dataset/test_reviews.csv\")\n",
    "\n",
    "# Convert tokens_str to lists\n",
    "def str_to_tokens(token_str):\n",
    "    try:\n",
    "        return eval(token_str) if isinstance(token_str, str) else token_str\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "train_df['tokens'] = train_df['tokens_str'].apply(str_to_tokens)\n",
    "val_df['tokens'] = val_df['tokens_str'].apply(str_to_tokens)\n",
    "test_df['tokens'] = test_df['tokens_str'].apply(str_to_tokens)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing if tokens are missing\n",
    "train_df['tokens'] = train_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "val_df['tokens'] = val_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "test_df['tokens'] = test_df.apply(lambda row: preprocess_text(row['text']) if not row['tokens'] else row['tokens'], axis=1)\n",
    "\n",
    "# Load hyperparameter results from Section 3.2\n",
    "results_df = pd.read_csv(\"dataset/hyperparameter_results.csv\")\n",
    "\n",
    "# Train embeddings\n",
    "def train_embeddings(tokens_list, embedding_type, mode, params):\n",
    "    vector_size = params['vector_size']\n",
    "    window = params['window']\n",
    "    epochs = params['epochs']\n",
    "    min_count = params['min_count']\n",
    "\n",
    "    if embedding_type == 'word2vec':\n",
    "        model = Word2Vec(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            workers=multiprocessing.cpu_count()\n",
    "        )\n",
    "    elif embedding_type == 'fasttext':\n",
    "        model = FastText(\n",
    "            sentences=tokens_list,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=1 if mode == 'sg' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            min_n=params['min_n'],\n",
    "            max_n=params['max_n'],\n",
    "            workers=multiprocessing.cpu_count()\n",
    "        )\n",
    "    elif embedding_type == 'doc2vec':\n",
    "        tagged_docs = [\n",
    "            TaggedDocument(words=tokens, tags=[f\"{row['user_id']}_{row['business_id']}\"])\n",
    "            for _, row in train_df.iterrows() for tokens in [row['tokens']]\n",
    "        ]\n",
    "        model = Doc2Vec(\n",
    "            tagged_docs,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            dm=1 if mode == 'dm' else 0,\n",
    "            epochs=epochs,\n",
    "            seed=1234,\n",
    "            alpha=params.get('alpha', 0.025),\n",
    "            min_alpha=params.get('alpha', 0.025) / 10,\n",
    "            workers=multiprocessing.cpu_count()\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid embedding type: {embedding_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get document embedding\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type):\n",
    "    if embedding_type == 'doc2vec':\n",
    "        return model.infer_vector(tokens, epochs=50, alpha=0.025)  # Changed 'steps' to 'epochs'\n",
    "    else:\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if not vectors:\n",
    "            return np.zeros(model.vector_size)\n",
    "        return np.mean(vectors, axis=0) if agg_type == 'average' else np.sum(vectors, axis=0)\n",
    "\n",
    "# Compute CCC (Concordance Correlation Coefficient)\n",
    "def compute_ccc(y_true, y_pred):\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    covar = np.cov(y_true, y_pred, bias=True)[0][1]\n",
    "    ccc = (2 * covar) / (var_true + var_pred + (mean_true - mean_pred) ** 2)\n",
    "    return ccc\n",
    "\n",
    "# Compute NDCG@k per user\n",
    "def compute_ndcg_per_user(y_true, y_pred, user_ids, k):\n",
    "    ndcg_scores = []\n",
    "    for user_id in np.unique(user_ids):\n",
    "        user_mask = user_ids == user_id\n",
    "        y_true_user = y_true[user_mask]\n",
    "        y_pred_user = y_pred[user_mask]\n",
    "        \n",
    "        if len(y_true_user) >= k:\n",
    "            sorted_indices = np.argsort(y_pred_user)[::-1]\n",
    "            y_true_sorted = y_true_user[sorted_indices]\n",
    "            y_pred_sorted = y_pred_user[sorted_indices]\n",
    "            ndcg = ndcg_score([y_true_sorted], [y_pred_sorted], k=k)\n",
    "            ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(setting, params, train_df, val_df, test_df):\n",
    "    embedding_type, mode = setting.split('-')\n",
    "    \n",
    "    # Train embeddings\n",
    "    model = train_embeddings(train_df['tokens'], embedding_type, mode, params)\n",
    "    \n",
    "    # Create document embeddings\n",
    "    train_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in train_df['tokens']])\n",
    "    val_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in val_df['tokens']])\n",
    "    test_features = np.array([get_doc_embedding(tokens, model, embedding_type, params['agg_type']) for tokens in test_df['tokens']])\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    val_features = scaler.transform(val_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    \n",
    "    # Train regressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_iter=params['max_iter'],\n",
    "        max_depth=params['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = regressor.predict(train_features)\n",
    "    val_pred = regressor.predict(val_features)\n",
    "    test_pred = regressor.predict(test_features)\n",
    "    \n",
    "    # Compute metrics\n",
    "    def compute_metrics(y_true, y_pred, user_ids):\n",
    "        metrics = {}\n",
    "        metrics['R2'] = r2_score(y_true, y_pred)\n",
    "        metrics['MAE'] = mean_absolute_error(y_true, y_pred)\n",
    "        metrics['RMSE'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        metrics['MAPE'] = MeanAbsolutePercentageError()(torch.tensor(y_pred), torch.tensor(y_true)).item()\n",
    "        metrics['CCC'] = compute_ccc(y_true, y_pred)\n",
    "        metrics['Pearson'] = pearsonr(y_true, y_pred)[0]\n",
    "        metrics['Spearman'] = spearmanr(y_true, y_pred)[0]\n",
    "        metrics['Kendall'] = kendalltau(y_true, y_pred)[0]\n",
    "        \n",
    "        # Compute NDCG@k per user\n",
    "        metrics['NDCG@10'] = compute_ndcg_per_user(y_true, y_pred, user_ids, k=10)\n",
    "        metrics['NDCG@20'] = compute_ndcg_per_user(y_true, y_pred, user_ids, k=20)\n",
    "        metrics['NDCG@50'] = compute_ndcg_per_user(y_true, y_pred, user_ids, k=50)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    results = {\n",
    "        'setting': setting,\n",
    "        'train': compute_metrics(train_df['stars'].values, train_pred, train_df['user_id'].values),\n",
    "        'val': compute_metrics(val_df['stars'].values, val_pred, val_df['user_id'].values),\n",
    "        'test': compute_metrics(test_df['stars'].values, test_pred, test_df['user_id'].values)\n",
    "    }\n",
    "    \n",
    "    # Free memory\n",
    "    del model, train_features, val_features, test_features, regressor\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all settings\n",
    "results = []\n",
    "for _, row in results_df.iterrows():\n",
    "    setting = row['setting']\n",
    "    params = row.to_dict()\n",
    "    del params['setting'], params['best_r2']  # Remove non-hyperparameter columns\n",
    "    result = evaluate_model(setting, params, train_df, val_df, test_df)\n",
    "    results.append(result)\n",
    "\n",
    "# Format results for output\n",
    "eval_results = []\n",
    "for result in results:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for metric, value in result[split].items():\n",
    "            eval_results.append({\n",
    "                'setting': result['setting'],\n",
    "                'split': split,\n",
    "                'metric': metric,\n",
    "                'value': value\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "eval_results_df = pd.DataFrame(eval_results)\n",
    "eval_results_df['value'] = eval_results_df['value'].round(2)  # Round all metrics to 2 decimal places\n",
    "eval_results_df.to_csv(\"dataset/evaluation_results.csv\", index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(eval_results_df.pivot_table(index=['setting', 'split'], columns='metric', values='value').to_string())\n",
    "\n",
    "# Visualization\n",
    "# 1. Bar Plot for R²\n",
    "r2_data = eval_results_df[eval_results_df['metric'] == 'R2']\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='setting', y='value', hue='split', data=r2_data)\n",
    "plt.title('R² Scores Across Settings and Splits')\n",
    "plt.xlabel('Setting')\n",
    "plt.ylabel('R²')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Split')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/r2_bar_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Line Plot for NDCG@k\n",
    "ndcg_data = eval_results_df[eval_results_df['metric'].isin(['NDCG@10', 'NDCG@20', 'NDCG@50'])]\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='setting', y='value', hue='metric', style='split', markers=True, data=ndcg_data)\n",
    "plt.title('NDCG@k Scores Across Settings and Splits')\n",
    "plt.xlabel('Setting')\n",
    "plt.ylabel('NDCG')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metric / Split')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/ndcg_line_plot.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nResults saved to 'dataset/evaluation_results.csv'\")\n",
    "print(\"Plots saved to 'figures/r2_bar_plot.png' and 'figures/ndcg_line_plot.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# Load validation data (ensure val_reviews.csv is in the dataset directory)\n",
    "val_df = pd.read_csv(\"dataset/val_reviews.csv\")\n",
    "\n",
    "# Define get_doc_embedding function (assuming it's from previous code)\n",
    "def get_doc_embedding(tokens, model, embedding_type, agg_type):\n",
    "    if embedding_type == 'doc2vec':\n",
    "        return model.infer_vector(tokens, epochs=50, alpha=0.025)\n",
    "    else:\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if not vectors:\n",
    "            return np.zeros(model.vector_size)\n",
    "        return np.mean(vectors, axis=0) if agg_type == 'average' else np.sum(vectors, axis=0)\n",
    "\n",
    "# Assume train_features and train_df are preloaded; load train data if needed\n",
    "train_df = pd.read_csv(\"dataset/train_reviews.csv\")\n",
    "train_features = np.array([get_doc_embedding(tokens, Word2Vec(sentences=train_df['tokens'], vector_size=100, window=5, min_count=1, sg=1, seed=1234), 'word2vec', 'average') \n",
    "                         for tokens in train_df['tokens']])\n",
    "train_features = StandardScaler().fit_transform(train_features)\n",
    "\n",
    "# Predict top restaurants for user Tashalee using the best model (Word2Vec-SGNS)\n",
    "tashalee_reviews = val_df[val_df['user_id'] == 'Tashalee']\n",
    "if not tashalee_reviews.empty:\n",
    "    # Load best parameters from Part 3.2 (assumed to be stored previously)\n",
    "    best_params_word2vec_sg = {\n",
    "        'vector_size': 100,  # Example; replace with actual value from hyperparameter_results.csv\n",
    "        'window': 5,        # Example; replace with actual value\n",
    "        'epochs': 10,       # Example; replace with actual value\n",
    "        'min_count': 1,     # Example; replace with actual value\n",
    "        'learning_rate': 0.1,  # Example; replace with actual value\n",
    "        'max_iter': 200,    # Example; replace with actual value\n",
    "        'max_depth': 5,     # Example; replace with actual value\n",
    "        'agg_type': 'average'\n",
    "    }\n",
    "\n",
    "    # Train Word2Vec model with optimized parameters\n",
    "    model = Word2Vec(\n",
    "        sentences=train_df['tokens'],\n",
    "        vector_size=best_params_word2vec_sg['vector_size'],\n",
    "        window=best_params_word2vec_sg['window'],\n",
    "        min_count=best_params_word2vec_sg['min_count'],\n",
    "        sg=1,  # SGNS mode\n",
    "        epochs=best_params_word2vec_sg['epochs'],\n",
    "        seed=1234,\n",
    "        workers=multiprocessing.cpu_count()\n",
    "    )\n",
    "\n",
    "    # Generate document embeddings for Tashalee's reviews\n",
    "    features = np.array([get_doc_embedding(tokens, model, 'word2vec', best_params_word2vec_sg['agg_type']) \n",
    "                        for tokens in tashalee_reviews['tokens']])\n",
    "    \n",
    "    # Normalize the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Train and predict with HistGradientBoostingRegressor\n",
    "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "    regressor = HistGradientBoostingRegressor(\n",
    "        learning_rate=best_params_word2vec_sg['learning_rate'],\n",
    "        max_iter=best_params_word2vec_sg['max_iter'],\n",
    "        max_depth=best_params_word2vec_sg['max_depth'],\n",
    "        random_state=1234\n",
    "    )\n",
    "    # Assume train_features and train_df['stars'] are precomputed\n",
    "    regressor.fit(train_features, train_df['stars'])\n",
    "    preds = regressor.predict(features)\n",
    "\n",
    "    # Sort restaurants by predicted score and select top 10\n",
    "    top_restaurant_indices = np.argsort(preds)[::-1][:10]\n",
    "    top_restaurants = tashalee_reviews.iloc[top_restaurant_indices]['business_id'].tolist()\n",
    "\n",
    "    # Print and save the results\n",
    "    print(\"\\nTop 10 restaurants for Tashalee (Word2Vec-SGNS):\", top_restaurants)\n",
    "    with open('dataset/top_restaurants_tashalee.txt', 'w') as f:\n",
    "        f.write(\"Top 10 restaurants for Tashalee (Word2Vec-SGNS):\\n\")\n",
    "        for i, rest_id in enumerate(top_restaurants, 1):\n",
    "            f.write(f\"{i}. {rest_id}\\n\")\n",
    "else:\n",
    "    print(\"No reviews found for user Tashalee in validation set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
